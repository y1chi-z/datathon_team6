{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import OrderedDict\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(\"/Users/yichi/Desktop/datathon/Data_Entry_2017.csv\")\n",
    "\n",
    "# Drop missing entries\n",
    "metadata = metadata.dropna(subset=[\"Patient Age\", \"Patient Gender\"])\n",
    "\n",
    "# Normalize age\n",
    "age_mean = metadata[\"Patient Age\"].mean()\n",
    "age_std = metadata[\"Patient Age\"].std()\n",
    "metadata[\"age_scaled\"] = (metadata[\"Patient Age\"] - age_mean) / age_std\n",
    "\n",
    "# Encode gender: F=0, M=1\n",
    "metadata[\"gender_encoded\"] = metadata[\"Patient Gender\"].map({\"F\": 0, \"M\": 1})\n",
    "\n",
    "# Create dictionary: image_name â†’ [age, gender]\n",
    "age_gender_dict = {\n",
    "    row[\"Image Index\"]: [row[\"age_scaled\"], row[\"gender_encoded\"]]\n",
    "    for _, row in metadata.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataSet(Dataset):\n",
    "    def __init__(self, data_dir, image_list_file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        image_names = []\n",
    "        labels = []\n",
    "        with open(image_list_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                items = line.split()\n",
    "                image_name = items[0]\n",
    "                label = [int(i) for i in items[1:]]\n",
    "                image_names.append(image_name)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, img_name, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayWithAux(Dataset):\n",
    "    def __init__(self, data_dir, image_list_file, age_gender_dict, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.age_gender_dict = age_gender_dict\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_names = []\n",
    "        self.labels = []\n",
    "\n",
    "        with open(image_list_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                self.image_names.append(parts[0])\n",
    "                self.labels.append([int(x) for x in parts[1:]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        aux = self.age_gender_dict.get(img_name, [0.0, 0])\n",
    "        aux_tensor = torch.tensor(aux, dtype=torch.float32)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, aux_tensor, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34169214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFusionModel(nn.Module):\n",
    "    def __init__(self, base_densenet_model, aux_input_dim=2, aux_hidden_dim=64, num_classes=14):\n",
    "        super(LatentFusionModel, self).__init__()\n",
    "\n",
    "        # Use pretrained DenseNet model (CheXNet) as encoder\n",
    "        self.features = base_densenet_model.features\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.cnn_output_dim = base_densenet_model.classifier.in_features  # 1024\n",
    "\n",
    "        # MLP for auxiliary data\n",
    "        self.aux_net = nn.Sequential(\n",
    "            nn.Linear(aux_input_dim, aux_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(aux_hidden_dim, aux_hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Classifier head after fusion\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.cnn_output_dim + aux_hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, aux_features):\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        aux_emb = self.aux_net(aux_features)\n",
    "        fused = torch.cat([x, aux_emb], dim=1)\n",
    "\n",
    "        return self.classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LIST = \"/Users/yichi/Desktop/datathon/train_list.txt\"\n",
    "IMAGE_DIR = \"/Users/yichi/Desktop/datathon/images\"\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ChestXrayWithAux(\n",
    "    data_dir=IMAGE_DIR,\n",
    "    image_list_file=TRAIN_LIST,\n",
    "    age_gender_dict=age_gender_dict,\n",
    "    transform=data_transforms\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yichi/miniforge3/envs/dsci572/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/yichi/miniforge3/envs/dsci572/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mf/5phxx_t14qn_kx9g6v8k0fj80000gn/T/ipykernel_61719/576734969.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(CKPT_PATH, map_location=torch.device(\"mps\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "=> loaded checkpoint\n"
     ]
    }
   ],
   "source": [
    "model = densenet121(pretrained=False)\n",
    "model.classifier = nn.Linear(1024, 14)\n",
    "\n",
    "CKPT_PATH = '/Users/yichi/Desktop/datathon/datathon_team6/model.pth.tar'\n",
    "\n",
    "if os.path.isfile(CKPT_PATH):\n",
    "    print(\"=> loading checkpoint\")\n",
    "    checkpoint = torch.load(CKPT_PATH, map_location=torch.device(\"mps\"))\n",
    "\n",
    "    modelCheckpoint = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
    "\n",
    "    new_state_dict = {}\n",
    "    for k in list(modelCheckpoint.keys()):\n",
    "        v = modelCheckpoint[k]\n",
    "\n",
    "        # Remove prefixes\n",
    "        if k.startswith(\"module.densenet121.\"):\n",
    "            k = k[len(\"module.densenet121.\"):]\n",
    "        elif k.startswith(\"module.\"):\n",
    "            k = k[len(\"module.\"):]\n",
    "\n",
    "        # Fix classifier.0.* â†’ classifier.*\n",
    "        if k.startswith(\"classifier.0.\"):\n",
    "            k = k.replace(\"classifier.0.\", \"classifier.\")\n",
    "\n",
    "        # Custom renaming logic (optional)\n",
    "        try:\n",
    "            index = k.rindex('.')\n",
    "            if k[index - 1] in ('1', '2'):\n",
    "                k = k[:index - 2] + k[index - 1:]\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    print(\"=> loaded checkpoint\")\n",
    "else:\n",
    "    print(\"=> no checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_model = LatentFusionModel(base_densenet_model=model).to(device)\n",
    "fusion_model = torch.nn.DataParallel(fusion_model).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fusion_model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    fusion_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, aux_features, labels in trainloader:\n",
    "        images = images.to(device)\n",
    "        aux_features = aux_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = fusion_model(images, aux_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(trainloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # # AUC EVALUATION (takes forever to run)\n",
    "    # fusion_model.eval()\n",
    "    # all_preds = []\n",
    "    # all_targets = []\n",
    "    # with torch.no_grad():\n",
    "    #     for images, aux_features, labels in trainloader:\n",
    "    #         images = images.to(device)\n",
    "    #         aux_features = aux_features.to(device)\n",
    "    #         labels = labels.to(device)\n",
    "\n",
    "    #         outputs = fusion_model(images, aux_features)\n",
    "    #         probs = torch.sigmoid(outputs)\n",
    "    #         all_preds.append(probs.cpu())\n",
    "    #         all_targets.append(labels.cpu())\n",
    "\n",
    "    # all_preds = torch.cat(all_preds).numpy()\n",
    "    # all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    # aucs = []\n",
    "    # for i in range(all_targets.shape[1]):\n",
    "    #     try:\n",
    "    #         auc = roc_auc_score(all_targets[:, i], all_preds[:, i])\n",
    "    #     except ValueError:\n",
    "    #         auc = float('nan')\n",
    "    #     aucs.append(auc)\n",
    "\n",
    "    # mean_auc = np.nanmean(aucs)\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}], Train AUC: {mean_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LIST = \"/Users/yichi/Desktop/datathon/test_list.txt\"\n",
    "\n",
    "test_dataset = ChestXrayWithAux(\n",
    "    data_dir=IMAGE_DIR,\n",
    "    image_list_file=TEST_LIST,\n",
    "    age_gender_dict=age_gender_dict,\n",
    "    transform=data_transforms\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci572",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
