{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import OrderedDict\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5641bb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata loaded for 112120 images.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(\"/Users/yichi/Desktop/datathon/Data_Entry_2017.csv\")\n",
    "\n",
    "# Drop rows with missing age or gender (if any)\n",
    "metadata = metadata.dropna(subset=[\"Patient Age\", \"Patient Gender\"])\n",
    "\n",
    "# Normalize age\n",
    "scaler = StandardScaler()\n",
    "metadata[\"age_scaled\"] = scaler.fit_transform(metadata[[\"Patient Age\"]])\n",
    "\n",
    "# Encode gender as binary (Female=0, Male=1)\n",
    "label_encoder = LabelEncoder()\n",
    "metadata[\"gender_encoded\"] = label_encoder.fit_transform(metadata[\"Patient Gender\"])\n",
    "\n",
    "# Optionally, store metadata in a dictionary for fast access\n",
    "patient_info = {\n",
    "    row[\"Image Index\"]: (row[\"age_scaled\"], row[\"gender_encoded\"])\n",
    "    for _, row in metadata.iterrows()\n",
    "}\n",
    "print(f\"Metadata loaded for {len(patient_info)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94eb106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class with early fusion\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_list, labels, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Early fusion: Get age and gender\n",
    "        age, gender = patient_info.get(img_name, (0.0, 0))  # fallback values\n",
    "\n",
    "        # Convert to tensor\n",
    "        extra_features = torch.tensor([age, gender], dtype=torch.float32)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, extra_features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUCs(gt, pred):\n",
    "    \"\"\"Computes Area Under the Curve (AUC) from prediction scores.\n",
    "\n",
    "    Args:\n",
    "        gt: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          true binary labels.\n",
    "        pred: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          can either be probability estimates of the positive class,\n",
    "          confidence values, or binary decisions.\n",
    "\n",
    "    Returns:\n",
    "        List of AUROCs of all classes.\n",
    "    \"\"\"\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(N_CLASSES):\n",
    "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "    return AUROCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "TRAIN_LIST = \"/Users/yichi/Desktop/datathon/train_list.txt\"\n",
    "IMAGE_DIR = \"/Users/yichi/Desktop/datathon/images\"\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = ChestXrayDataSet(IMAGE_DIR, TRAIN_LIST, transform=data_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([64, 3, 224, 224])\n",
      "Labels shape: torch.Size([64, 14])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "print(\"Images shape:\", images.shape) \n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = '/Users/yichi/Desktop/datathon/model.pth.tar'\n",
    "N_CLASSES = 14\n",
    "CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "DATA_DIR = '/Users/yichi/Desktop/datathon/images'\n",
    "TEST_IMAGE_LIST = '/Users/yichi/Desktop/datathon/test_list.txt'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mf/5phxx_t14qn_kx9g6v8k0fj80000gn/T/ipykernel_15331/2492410187.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(CKPT_PATH, map_location=torch.device(\"mps\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint\n"
     ]
    }
   ],
   "source": [
    "model = DenseNet121(N_CLASSES).to(device)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "if os.path.isfile(CKPT_PATH):\n",
    "    print(\"=> loading checkpoint\")\n",
    "    checkpoint = torch.load(CKPT_PATH, map_location=torch.device(\"mps\"))\n",
    "\n",
    "    # If it's a full checkpoint with 'state_dict', extract it\n",
    "    if 'state_dict' in checkpoint:\n",
    "        modelCheckpoint = checkpoint['state_dict']\n",
    "    else:\n",
    "        modelCheckpoint = checkpoint\n",
    "\n",
    "    # Fix any key renaming (only if needed)\n",
    "    new_state_dict = {}\n",
    "    for k in list(modelCheckpoint.keys()):\n",
    "        try:\n",
    "            index = k.rindex('.')\n",
    "            if k[index - 1] in ('1', '2'):\n",
    "                new_key = k[:index - 2] + k[index - 1:]\n",
    "                new_state_dict[new_key] = modelCheckpoint[k]\n",
    "            else:\n",
    "                new_state_dict[k] = modelCheckpoint[k]\n",
    "        except ValueError:\n",
    "            new_state_dict[k] = modelCheckpoint[k]\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    print(\"=> loaded checkpoint\")\n",
    "else:\n",
    "    print(\"=> no checkpoint found\")\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                [0.229, 0.224, 0.225])\n",
    "\n",
    "test_dataset = ChestXrayDataSet(data_dir=DATA_DIR,\n",
    "                                    image_list_file=TEST_IMAGE_LIST,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.Resize(256),\n",
    "                                        transforms.TenCrop(224),\n",
    "                                        transforms.Lambda\n",
    "                                        (lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                        transforms.Lambda\n",
    "                                        (lambda crops: torch.stack([normalize(crop) for crop in crops]))\n",
    "                                    ]))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Average AUROC: 0.883\n",
      "AUROC for Atelectasis: 0.861\n",
      "AUROC for Cardiomegaly: 0.964\n",
      "AUROC for Effusion: 0.920\n",
      "AUROC for Infiltration: 0.759\n",
      "AUROC for Mass: 0.915\n",
      "AUROC for Nodule: 0.797\n",
      "AUROC for Pneumonia: 0.769\n",
      "AUROC for Pneumothorax: 0.927\n",
      "AUROC for Consolidation: 0.865\n",
      "AUROC for Edema: 0.948\n",
      "AUROC for Emphysema: 0.955\n",
      "AUROC for Fibrosis: 0.869\n",
      "AUROC for Pleural_Thickening: 0.826\n",
      "AUROC for Hernia: 0.988\n"
     ]
    }
   ],
   "source": [
    "# gt = torch.FloatTensor()\n",
    "# gt = gt.to(device)\n",
    "# pred = torch.FloatTensor()\n",
    "# pred = pred.to(device)\n",
    "\n",
    "# # switch to evaluate mode\n",
    "# model.eval()\n",
    "\n",
    "# for i, (inp, target) in enumerate(test_loader):\n",
    "#     target = target.to(device)\n",
    "#     gt = torch.cat((gt, target), 0)\n",
    "#     bs, n_crops, c, h, w = inp.size()\n",
    "#     input_var = torch.autograd.Variable(inp.view(-1, c, h, w).to(device), volatile=True)\n",
    "#     output = model(input_var)\n",
    "#     output_mean = output.view(bs, n_crops, -1).mean(1)\n",
    "#     pred = torch.cat((pred, output_mean.data), 0)\n",
    "\n",
    "# AUROCs = compute_AUCs(gt, pred)\n",
    "# AUROC_avg = np.array(AUROCs).mean()\n",
    "# print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "# for i in range(N_CLASSES):\n",
    "#     print('The AUROC of {} is {}'.format(CLASS_NAMES[i], AUROCs[i]))\n",
    "\n",
    "gt = torch.FloatTensor().to(device)\n",
    "pred = torch.FloatTensor().to(device)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for i, (inp, target) in enumerate(test_loader):\n",
    "    target = target.to(device)\n",
    "    gt = torch.cat((gt, target), dim=0)\n",
    "\n",
    "    bs, n_crops, c, h, w = inp.size()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_var = inp.view(-1, c, h, w).to(device)\n",
    "        output = model(input_var)\n",
    "\n",
    "    output_mean = output.view(bs, n_crops, -1).mean(1)\n",
    "    pred = torch.cat((pred, output_mean), dim=0)\n",
    "\n",
    "# Evaluate AUROC\n",
    "AUROCs = compute_AUCs(gt, pred)\n",
    "AUROC_avg = np.array(AUROCs).mean()\n",
    "\n",
    "print(f'\\n✅ Average AUROC: {AUROC_avg:.3f}')\n",
    "for i in range(N_CLASSES):\n",
    "    print(f'AUROC for {CLASS_NAMES[i]}: {AUROCs[i]:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci572",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
