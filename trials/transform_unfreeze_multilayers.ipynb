{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Augmentation, Fine-Tuning Last Dense Block, Transition Layer, Classifier\n",
    "\n",
    "> In this notebook, we will only apply **augmentation**, **fine-tune the last dense block**, **transition layer**, and **classifier**. We'll use this approach as a comparison with other notebooks that use different techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = 'model.pth.tar'\n",
    "N_CLASSES = 14\n",
    "CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "DATA_DIR = 'images'\n",
    "TEST_IMAGE_LIST = 'test_list.txt'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataSet(Dataset):\n",
    "    def __init__(self, data_dir, image_list_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: path to image directory.\n",
    "            image_list_file: path to the file containing images\n",
    "                with corresponding labels.\n",
    "            transform: optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        image_names = []\n",
    "        labels = []\n",
    "        with open(image_list_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                items = line.split()\n",
    "                image_name= items[0]\n",
    "                label = items[1:]\n",
    "                label = [int(i) for i in label]\n",
    "                image_name = os.path.join(data_dir, image_name)\n",
    "                image_names.append(image_name)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index: the index of item\n",
    "\n",
    "        Returns:\n",
    "            image and its labels\n",
    "        \"\"\"\n",
    "        image_name = self.image_names[index]\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUCs(gt, pred):\n",
    "    \"\"\"Computes Area Under the Curve (AUC) from prediction scores.\n",
    "\n",
    "    Args:\n",
    "        gt: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          true binary labels.\n",
    "        pred: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          can either be probability estimates of the positive class,\n",
    "          confidence values, or binary decisions.\n",
    "\n",
    "    Returns:\n",
    "        List of AUROCs of all classes.\n",
    "    \"\"\"\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(N_CLASSES):\n",
    "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "    return AUROCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "TRAIN_LIST = \"train_list.txt\"\n",
    "VALID_LIST = \"val_list.txt\"\n",
    "IMAGE_DIR = \"images\"\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = ChestXrayDataSet(IMAGE_DIR, TRAIN_LIST, transform=data_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([64, 3, 224, 224])\n",
      "Labels shape: torch.Size([64, 14])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "print(\"Images shape:\", images.shape) \n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aroce\\miniforge3\\envs\\572\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aroce\\miniforge3\\envs\\572\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "=> loaded checkpoint\n"
     ]
    }
   ],
   "source": [
    "model = DenseNet121(N_CLASSES).to(device)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "if os.path.isfile(CKPT_PATH):\n",
    "    print(\"=> loading checkpoint\")\n",
    "    modelCheckpoint = torch.load(CKPT_PATH)['state_dict']\n",
    "    for k in list(modelCheckpoint.keys()):\n",
    "        index = k.rindex('.')\n",
    "        if (k[index - 1] == '1' or k[index - 1] == '2'):\n",
    "            modelCheckpoint[k[:index - 2] + k[index - 1:]] = modelCheckpoint[k]\n",
    "            del modelCheckpoint[k]\n",
    "    model.load_state_dict(modelCheckpoint)\n",
    "    print(\"=> loaded checkpoint\")\n",
    "else:\n",
    "    print(\"=> no checkpoint found\")\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                [0.229, 0.224, 0.225])\n",
    "\n",
    "test_dataset = ChestXrayDataSet(data_dir=DATA_DIR,\n",
    "                                    image_list_file=TEST_IMAGE_LIST,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.Resize(256),\n",
    "                                        transforms.TenCrop(224),\n",
    "                                        transforms.Lambda\n",
    "                                        (lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                        transforms.Lambda\n",
    "                                        (lambda crops: torch.stack([normalize(crop) for crop in crops]))\n",
    "                                    ]))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class counts and weights\n",
    "class_counts = [\n",
    "    313,  # Atelectasis\n",
    "    141,  # Cardiomegaly\n",
    "    341,  # Effusion\n",
    "    580,  # Infiltration\n",
    "    111,  # Mass\n",
    "    151,  # Nodule\n",
    "    45,   # Pneumonia\n",
    "    141,  # Pneumothorax\n",
    "    136,  # Consolidation\n",
    "    62,   # Edema\n",
    "    86,   # Emphysema\n",
    "    117,  # Fibrosis\n",
    "    114,  # Pleural_Thickening\n",
    "    17    # Hernia\n",
    "]\n",
    "\n",
    "total_count = sum(class_counts)\n",
    "class_weights = [total_count / count for count in class_counts]\n",
    "class_weights_cpu = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weights = []\n",
    "for _, label in train_dataset:\n",
    "    label = label.float()\n",
    "    weight = torch.sum(class_weights_cpu * label).item()\n",
    "    sample_weights.append(weight)\n",
    "\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float)\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "class_weights_gpu = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced classifier with a regularized version\n"
     ]
    }
   ],
   "source": [
    "# Replace the classifier with a more regularized version\n",
    "old_classifier = model.module.densenet121.classifier\n",
    "num_ftrs = None\n",
    "if isinstance(old_classifier, nn.Sequential):\n",
    "    # Get input features from the first Linear layer\n",
    "    for layer in old_classifier:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            num_ftrs = layer.in_features\n",
    "            break\n",
    "else:\n",
    "    # Direct linear layer\n",
    "    num_ftrs = old_classifier.in_features\n",
    "\n",
    "model.module.densenet121.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, 1024),\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, N_CLASSES),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "print(\"Replaced classifier with a regularized version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 4,397,326\n"
     ]
    }
   ],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.module.densenet121.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classifier\n",
    "for param in model.module.densenet121.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Also unfreeze the last dense block (denseblock4)\n",
    "for param in model.module.densenet121.features.denseblock4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Also unfreeze the transition layer before denseblock4\n",
    "for param in model.module.densenet121.features.transition3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in trainable_params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Add weight decay to combat overfitting\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = ChestXrayDataSet(IMAGE_DIR, TRAIN_LIST, transform=train_transforms)\n",
    "valid_dataset = ChestXrayDataSet(IMAGE_DIR, VALID_LIST, transform=valid_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = DataLoader(train_dataset, sampler=sampler, batch_size=64, pin_memory=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=64, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=20, patience=5, verbose=True):\n",
    "    train_loss, valid_loss, valid_accuracy = [], [], []\n",
    "    \n",
    "    # To track best model\n",
    "    best_auroc = 0.0\n",
    "    best_model_weights = None\n",
    "    counter = 0  # For early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)  \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss.append(epoch_train_loss / len(trainloader.dataset))\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        epoch_valid_loss = 0.0\n",
    "        all_labels = torch.FloatTensor().to(device)\n",
    "        all_outputs = torch.FloatTensor().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                epoch_valid_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                all_labels = torch.cat((all_labels, labels), 0)\n",
    "                all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "                \n",
    "        # Calculate metrics\n",
    "        predictions = (all_outputs > 0.5).float()\n",
    "        correct = (predictions == all_labels).sum().item()\n",
    "        total = all_labels.numel()\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        aurocs = compute_AUCs(all_labels, all_outputs)\n",
    "        mean_auroc = np.mean(aurocs)\n",
    "        \n",
    "        valid_loss.append(epoch_valid_loss / len(validloader.dataset))\n",
    "        valid_accuracy.append(accuracy)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss[-1]:.4f}, Valid Loss: {valid_loss[-1]:.4f}\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}, Mean AUROC: {mean_auroc:.4f}\")\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "                for i, auroc in enumerate(aurocs):\n",
    "                    print(f\"  {CLASS_NAMES[i]}: AUROC = {auroc:.4f}\")\n",
    "        \n",
    "        # Early stopping and model saving\n",
    "        if mean_auroc > best_auroc:\n",
    "            best_auroc = mean_auroc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            print(f\"  New best model with AUROC: {best_auroc:.4f}\")\n",
    "            counter = 0  # Reset counter\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"  Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    # Load the best model weights\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print(f\"Loaded best model with AUROC: {best_auroc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'train_loss': train_loss, \n",
    "        'valid_loss': valid_loss, \n",
    "        'valid_accuracy': valid_accuracy,\n",
    "        'best_auroc': best_auroc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.5966, Valid Loss: 0.5565\n",
      "Accuracy: 0.8283, Mean AUROC: 0.7485\n",
      "  New best model with AUROC: 0.7485\n",
      "Epoch 2/20 - Train Loss: 0.4754, Valid Loss: 0.4628\n",
      "Accuracy: 0.8998, Mean AUROC: 0.7735\n",
      "  New best model with AUROC: 0.7735\n",
      "Epoch 3/20 - Train Loss: 0.4166, Valid Loss: 0.3836\n",
      "Accuracy: 0.9296, Mean AUROC: 0.7821\n",
      "  New best model with AUROC: 0.7821\n",
      "Epoch 4/20 - Train Loss: 0.3860, Valid Loss: 0.3253\n",
      "Accuracy: 0.9321, Mean AUROC: 0.7853\n",
      "  New best model with AUROC: 0.7853\n",
      "Epoch 5/20 - Train Loss: 0.3663, Valid Loss: 0.2867\n",
      "Accuracy: 0.9349, Mean AUROC: 0.7979\n",
      "  Atelectasis: AUROC = 0.7845\n",
      "  Cardiomegaly: AUROC = 0.9359\n",
      "  Effusion: AUROC = 0.8918\n",
      "  Infiltration: AUROC = 0.6144\n",
      "  Mass: AUROC = 0.7792\n",
      "  Nodule: AUROC = 0.5870\n",
      "  Pneumonia: AUROC = 0.7316\n",
      "  Pneumothorax: AUROC = 0.8771\n",
      "  Consolidation: AUROC = 0.7660\n",
      "  Edema: AUROC = 0.8860\n",
      "  Emphysema: AUROC = 0.8929\n",
      "  Fibrosis: AUROC = 0.7356\n",
      "  Pleural_Thickening: AUROC = 0.7223\n",
      "  Hernia: AUROC = 0.9664\n",
      "  New best model with AUROC: 0.7979\n",
      "Epoch 6/20 - Train Loss: 0.3563, Valid Loss: 0.2608\n",
      "Accuracy: 0.9349, Mean AUROC: 0.8050\n",
      "  New best model with AUROC: 0.8050\n",
      "Epoch 7/20 - Train Loss: 0.3463, Valid Loss: 0.2464\n",
      "Accuracy: 0.9357, Mean AUROC: 0.8032\n",
      "Epoch 8/20 - Train Loss: 0.3379, Valid Loss: 0.2349\n",
      "Accuracy: 0.9346, Mean AUROC: 0.8030\n",
      "Epoch 9/20 - Train Loss: 0.3336, Valid Loss: 0.2272\n",
      "Accuracy: 0.9330, Mean AUROC: 0.8035\n",
      "Epoch 10/20 - Train Loss: 0.3275, Valid Loss: 0.2220\n",
      "Accuracy: 0.9342, Mean AUROC: 0.8006\n",
      "  Atelectasis: AUROC = 0.7756\n",
      "  Cardiomegaly: AUROC = 0.9144\n",
      "  Effusion: AUROC = 0.8953\n",
      "  Infiltration: AUROC = 0.6231\n",
      "  Mass: AUROC = 0.8531\n",
      "  Nodule: AUROC = 0.6014\n",
      "  Pneumonia: AUROC = 0.7590\n",
      "  Pneumothorax: AUROC = 0.8651\n",
      "  Consolidation: AUROC = 0.7486\n",
      "  Edema: AUROC = 0.8553\n",
      "  Emphysema: AUROC = 0.8983\n",
      "  Fibrosis: AUROC = 0.7029\n",
      "  Pleural_Thickening: AUROC = 0.7246\n",
      "  Hernia: AUROC = 0.9921\n",
      "Epoch 11/20 - Train Loss: 0.3320, Valid Loss: 0.2197\n",
      "Accuracy: 0.9323, Mean AUROC: 0.8045\n",
      "  Early stopping triggered after 11 epochs\n",
      "Loaded best model with AUROC: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.5965555200204743,\n",
       "  0.47537220895682175,\n",
       "  0.4166333998278503,\n",
       "  0.3859797182697063,\n",
       "  0.3662604146103888,\n",
       "  0.3563324729096177,\n",
       "  0.34631310171248336,\n",
       "  0.3378532878004779,\n",
       "  0.33355066670830436,\n",
       "  0.32749856841090613,\n",
       "  0.33195252366050987],\n",
       " 'valid_loss': [0.5564775163332621,\n",
       "  0.4627814706961314,\n",
       "  0.38360670224825544,\n",
       "  0.3252780273755391,\n",
       "  0.2866893316109975,\n",
       "  0.26078374139467875,\n",
       "  0.24642280383904774,\n",
       "  0.23487487602233886,\n",
       "  0.22722331881523133,\n",
       "  0.2219768660068512,\n",
       "  0.2197488392194112],\n",
       " 'valid_accuracy': [0.8282857142857143,\n",
       "  0.8998095238095238,\n",
       "  0.9296190476190476,\n",
       "  0.9320952380952381,\n",
       "  0.9348571428571428,\n",
       "  0.9348571428571428,\n",
       "  0.9357142857142857,\n",
       "  0.9345714285714286,\n",
       "  0.932952380952381,\n",
       "  0.9341904761904762,\n",
       "  0.9322857142857143],\n",
       " 'best_auroc': np.float64(0.80495699487945)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "trainer(model, criterion, optimizer, trainloader, validloader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Average AUROC: 0.803\n",
      "AUROC for Atelectasis: 0.794\n",
      "AUROC for Cardiomegaly: 0.876\n",
      "AUROC for Effusion: 0.910\n",
      "AUROC for Infiltration: 0.658\n",
      "AUROC for Mass: 0.848\n",
      "AUROC for Nodule: 0.558\n",
      "AUROC for Pneumonia: 0.654\n",
      "AUROC for Pneumothorax: 0.921\n",
      "AUROC for Consolidation: 0.804\n",
      "AUROC for Edema: 0.917\n",
      "AUROC for Emphysema: 0.850\n",
      "AUROC for Fibrosis: 0.796\n",
      "AUROC for Pleural_Thickening: 0.653\n",
      "AUROC for Hernia: 0.996\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "gt = torch.FloatTensor().to(device)\n",
    "pred = torch.FloatTensor().to(device)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for i, (inp, target) in enumerate(test_loader):\n",
    "    target = target.to(device)\n",
    "    gt = torch.cat((gt, target), dim=0)\n",
    "\n",
    "    bs, n_crops, c, h, w = inp.size()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_var = inp.view(-1, c, h, w).to(device)\n",
    "        output = model(input_var)\n",
    "\n",
    "    output_mean = output.view(bs, n_crops, -1).mean(1)\n",
    "    pred = torch.cat((pred, output_mean), dim=0)\n",
    "\n",
    "# Evaluate AUROC\n",
    "AUROCs = compute_AUCs(gt, pred)\n",
    "AUROC_avg = np.array(AUROCs).mean()\n",
    "\n",
    "print(f'\\n✅ Average AUROC: {AUROC_avg:.3f}')\n",
    "for i in range(N_CLASSES):\n",
    "    print(f'AUROC for {CLASS_NAMES[i]}: {AUROCs[i]:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
