{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Metadata, Applying Augmentation, and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = 'model.pth.tar'\n",
    "N_CLASSES = 14\n",
    "CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "DATA_DIR = 'images'\n",
    "TEST_IMAGE_LIST = 'labels/test_list.txt'\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_LIST = \"labels/train_list.txt\"\n",
    "VALID_LIST = \"labels/val_list.txt\"\n",
    "IMAGE_DIR = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata loaded for 112120 images.\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(\"Data_Entry_2017.csv\")\n",
    "\n",
    "# Drop rows with missing age or gender (if any)\n",
    "metadata = metadata.dropna(subset=[\"Patient Age\", \"Patient Gender\"])\n",
    "\n",
    "# Normalize age\n",
    "scaler = StandardScaler()\n",
    "metadata[\"age_scaled\"] = scaler.fit_transform(metadata[[\"Patient Age\"]])\n",
    "\n",
    "# Encode gender as binary (Female=0, Male=1)\n",
    "label_encoder = LabelEncoder()\n",
    "metadata[\"gender_encoded\"] = label_encoder.fit_transform(metadata[\"Patient Gender\"])\n",
    "\n",
    "# Create metadata dictionary\n",
    "patient_info = {\n",
    "    row[\"Image Index\"]: (row[\"age_scaled\"], row[\"gender_encoded\"])\n",
    "    for _, row in metadata.iterrows()\n",
    "}\n",
    "print(f\"Metadata loaded for {len(patient_info)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataSet(Dataset):\n",
    "    def __init__(self, data_dir, image_list_file, metadata, transform=None):\n",
    "        image_names, labels = [], []\n",
    "        with open(image_list_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                items = line.split()\n",
    "                image_name = items[0]\n",
    "                label = [int(i) for i in items[1:]]\n",
    "                image_name = os.path.join(data_dir, image_name)\n",
    "                image_names.append(image_name)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_names[index]\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "\n",
    "        base_name = os.path.basename(image_name)\n",
    "        age, gender = self.metadata.get(base_name, (0.0, 0.0))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        age_tensor = torch.tensor([age], dtype=torch.float32)\n",
    "        gender_tensor = torch.tensor([gender], dtype=torch.float32)\n",
    "\n",
    "        return image, torch.FloatTensor(label), age_tensor, gender_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121WithMetadata(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121WithMetadata, self).__init__()\n",
    "        # Load pretrained DenseNet\n",
    "        self.densenet = models.densenet121(pretrained=True)\n",
    "        \n",
    "        # Get the feature size\n",
    "        self.feature_size = self.densenet.classifier.in_features\n",
    "        \n",
    "        # Remove the original classifier\n",
    "        self.densenet.classifier = nn.Identity()\n",
    "        \n",
    "        # Global pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Create a new classifier that takes image features + metadata\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.feature_size + 2, 1024),  # +2 for age and gender\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, age, gender):\n",
    "        # Extract features\n",
    "        features = self.densenet.features(x)\n",
    "        features = torch.relu(features)\n",
    "        features = self.avgpool(features)\n",
    "        features = torch.flatten(features, 1)\n",
    "        \n",
    "        # Combine metadata\n",
    "        metadata = torch.cat([age, gender], dim=1)\n",
    "        combined = torch.cat([features, metadata], dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUCs(gt, pred):\n",
    "    \"\"\"Computes Area Under the Curve (AUC) from prediction scores.\n",
    "\n",
    "    Args:\n",
    "        gt: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          true binary labels.\n",
    "        pred: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          can either be probability estimates of the positive class,\n",
    "          confidence values, or binary decisions.\n",
    "\n",
    "    Returns:\n",
    "        List of AUROCs of all classes.\n",
    "    \"\"\"\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(N_CLASSES):\n",
    "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "    return AUROCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChestXrayDataSet(IMAGE_DIR, TRAIN_LIST, metadata=patient_info, transform=train_transforms)\n",
    "valid_dataset = ChestXrayDataSet(IMAGE_DIR, VALID_LIST, metadata=patient_info, transform=valid_transforms)\n",
    "\n",
    "# Class counts in the same order as CLASS_NAMES\n",
    "class_counts = [\n",
    "    313,  # Atelectasis\n",
    "    141,  # Cardiomegaly\n",
    "    341,  # Effusion\n",
    "    580,  # Infiltration\n",
    "    111,  # Mass\n",
    "    151,  # Nodule\n",
    "    45,   # Pneumonia\n",
    "    141,  # Pneumothorax\n",
    "    136,  # Consolidation\n",
    "    62,   # Edema\n",
    "    86,   # Emphysema\n",
    "    117,  # Fibrosis\n",
    "    114,  # Pleural_Thickening\n",
    "    17    # Hernia\n",
    "]\n",
    "\n",
    "# Total samples and class weights\n",
    "total_count = sum(class_counts)\n",
    "class_weights = [total_count / count for count in class_counts]\n",
    "\n",
    "# Keep weights on CPU for computing sample weights\n",
    "class_weights_cpu = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "sample_weights = []\n",
    "for data_tuple in train_dataset:\n",
    "    label = data_tuple[1].float()\n",
    "    weight = torch.sum(class_weights_cpu * label).item()\n",
    "    sample_weights.append(weight)\n",
    "\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.float)\n",
    "\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, sampler=sampler, batch_size=64, pin_memory=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=64, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aroce\\miniforge3\\envs\\572\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aroce\\miniforge3\\envs\\572\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 4,399,374\n"
     ]
    }
   ],
   "source": [
    "new_model = DenseNet121WithMetadata(N_CLASSES).to(device)\n",
    "\n",
    "for param in new_model.densenet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Unfreeze last dense block and transition\n",
    "for param in new_model.densenet.features.denseblock4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in new_model.densenet.features.transition3.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in new_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainable_params = [p for p in new_model.parameters() if p.requires_grad]\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, new_model.parameters()), \n",
    "    lr=1e-4, weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=20, patience=5, verbose=True):\n",
    "    train_loss, valid_loss, valid_accuracy = [], [], []\n",
    "    best_auroc = 0.0\n",
    "    best_model_weights = None\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "\n",
    "        for images, labels, ages, genders in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            ages, genders = ages.to(device), genders.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, ages, genders)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss.append(epoch_train_loss / len(trainloader.dataset))\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        epoch_valid_loss = 0.0\n",
    "        all_labels = torch.FloatTensor().to(device)\n",
    "        all_outputs = torch.FloatTensor().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, ages, genders in validloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                ages, genders = ages.to(device), genders.to(device)\n",
    "\n",
    "                outputs = model(images, ages, genders)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_valid_loss += loss.item() * images.size(0)\n",
    "\n",
    "                all_labels = torch.cat((all_labels, labels), 0)\n",
    "                all_outputs = torch.cat((all_outputs, outputs), 0)\n",
    "\n",
    "        # Calculate metrics\n",
    "        predictions = (all_outputs > 0.5).float()\n",
    "        correct = (predictions == all_labels).sum().item()\n",
    "        total = all_labels.numel()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        aurocs = compute_AUCs(all_labels, all_outputs)\n",
    "        mean_auroc = np.mean(aurocs)\n",
    "\n",
    "        valid_loss.append(epoch_valid_loss / len(validloader.dataset))\n",
    "        valid_accuracy.append(accuracy)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss[-1]:.4f}, Valid Loss: {valid_loss[-1]:.4f}\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}, Mean AUROC: {mean_auroc:.4f}\")\n",
    "\n",
    "            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "                for i, auroc in enumerate(aurocs):\n",
    "                    print(f\"  {CLASS_NAMES[i]}: AUROC = {auroc:.4f}\")\n",
    "\n",
    "        # Early stopping and model saving\n",
    "        if mean_auroc > best_auroc:\n",
    "            best_auroc = mean_auroc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            print(f\"  New best model with AUROC: {best_auroc:.4f}\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"  Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    # Load the best model weights\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print(f\"Loaded best model with AUROC: {best_auroc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'train_loss': train_loss, \n",
    "        'valid_loss': valid_loss, \n",
    "        'valid_accuracy': valid_accuracy,\n",
    "        'best_auroc': best_auroc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.6150, Valid Loss: 0.5667\n",
      "Accuracy: 0.9497, Mean AUROC: 0.6157\n",
      "  New best model with AUROC: 0.6157\n",
      "Epoch 2/50 - Train Loss: 0.4905, Valid Loss: 0.4570\n",
      "Accuracy: 0.9508, Mean AUROC: 0.6442\n",
      "  New best model with AUROC: 0.6442\n",
      "Epoch 3/50 - Train Loss: 0.4239, Valid Loss: 0.3713\n",
      "Accuracy: 0.9498, Mean AUROC: 0.6760\n",
      "  New best model with AUROC: 0.6760\n",
      "Epoch 4/50 - Train Loss: 0.3857, Valid Loss: 0.3343\n",
      "Accuracy: 0.9508, Mean AUROC: 0.6898\n",
      "  New best model with AUROC: 0.6898\n",
      "Epoch 5/50 - Train Loss: 0.3634, Valid Loss: 0.2942\n",
      "Accuracy: 0.9499, Mean AUROC: 0.7046\n",
      "  Atelectasis: AUROC = 0.6619\n",
      "  Cardiomegaly: AUROC = 0.6726\n",
      "  Effusion: AUROC = 0.7833\n",
      "  Infiltration: AUROC = 0.5958\n",
      "  Mass: AUROC = 0.6043\n",
      "  Nodule: AUROC = 0.5362\n",
      "  Pneumonia: AUROC = 0.7455\n",
      "  Pneumothorax: AUROC = 0.8035\n",
      "  Consolidation: AUROC = 0.7213\n",
      "  Edema: AUROC = 0.8163\n",
      "  Emphysema: AUROC = 0.6511\n",
      "  Fibrosis: AUROC = 0.7172\n",
      "  Pleural_Thickening: AUROC = 0.6729\n",
      "  Hernia: AUROC = 0.8821\n",
      "  New best model with AUROC: 0.7046\n",
      "Epoch 6/50 - Train Loss: 0.3533, Valid Loss: 0.2782\n",
      "Accuracy: 0.9492, Mean AUROC: 0.7105\n",
      "  New best model with AUROC: 0.7105\n",
      "Epoch 7/50 - Train Loss: 0.3408, Valid Loss: 0.2593\n",
      "Accuracy: 0.9489, Mean AUROC: 0.7114\n",
      "  New best model with AUROC: 0.7114\n",
      "Epoch 8/50 - Train Loss: 0.3341, Valid Loss: 0.2519\n",
      "Accuracy: 0.9481, Mean AUROC: 0.7204\n",
      "  New best model with AUROC: 0.7204\n",
      "Epoch 9/50 - Train Loss: 0.3192, Valid Loss: 0.2443\n",
      "Accuracy: 0.9465, Mean AUROC: 0.7174\n",
      "Epoch 10/50 - Train Loss: 0.3180, Valid Loss: 0.2333\n",
      "Accuracy: 0.9471, Mean AUROC: 0.7218\n",
      "  Atelectasis: AUROC = 0.6849\n",
      "  Cardiomegaly: AUROC = 0.7513\n",
      "  Effusion: AUROC = 0.8118\n",
      "  Infiltration: AUROC = 0.6088\n",
      "  Mass: AUROC = 0.6355\n",
      "  Nodule: AUROC = 0.5418\n",
      "  Pneumonia: AUROC = 0.6996\n",
      "  Pneumothorax: AUROC = 0.8191\n",
      "  Consolidation: AUROC = 0.7208\n",
      "  Edema: AUROC = 0.8056\n",
      "  Emphysema: AUROC = 0.7062\n",
      "  Fibrosis: AUROC = 0.7173\n",
      "  Pleural_Thickening: AUROC = 0.6455\n",
      "  Hernia: AUROC = 0.9564\n",
      "  New best model with AUROC: 0.7218\n",
      "Epoch 11/50 - Train Loss: 0.3075, Valid Loss: 0.2285\n",
      "Accuracy: 0.9457, Mean AUROC: 0.7198\n",
      "Epoch 12/50 - Train Loss: 0.3004, Valid Loss: 0.2241\n",
      "Accuracy: 0.9474, Mean AUROC: 0.7236\n",
      "  New best model with AUROC: 0.7236\n",
      "Epoch 13/50 - Train Loss: 0.2943, Valid Loss: 0.2213\n",
      "Accuracy: 0.9444, Mean AUROC: 0.7223\n",
      "Epoch 14/50 - Train Loss: 0.2920, Valid Loss: 0.2191\n",
      "Accuracy: 0.9477, Mean AUROC: 0.7256\n",
      "  New best model with AUROC: 0.7256\n",
      "Epoch 15/50 - Train Loss: 0.2847, Valid Loss: 0.2201\n",
      "Accuracy: 0.9416, Mean AUROC: 0.7226\n",
      "  Atelectasis: AUROC = 0.6794\n",
      "  Cardiomegaly: AUROC = 0.7554\n",
      "  Effusion: AUROC = 0.8127\n",
      "  Infiltration: AUROC = 0.6143\n",
      "  Mass: AUROC = 0.6082\n",
      "  Nodule: AUROC = 0.5192\n",
      "  Pneumonia: AUROC = 0.7357\n",
      "  Pneumothorax: AUROC = 0.8308\n",
      "  Consolidation: AUROC = 0.7113\n",
      "  Edema: AUROC = 0.8265\n",
      "  Emphysema: AUROC = 0.7140\n",
      "  Fibrosis: AUROC = 0.6902\n",
      "  Pleural_Thickening: AUROC = 0.6559\n",
      "  Hernia: AUROC = 0.9635\n",
      "Epoch 16/50 - Train Loss: 0.2762, Valid Loss: 0.2225\n",
      "Accuracy: 0.9428, Mean AUROC: 0.7286\n",
      "  New best model with AUROC: 0.7286\n",
      "Epoch 17/50 - Train Loss: 0.2716, Valid Loss: 0.2296\n",
      "Accuracy: 0.9390, Mean AUROC: 0.7343\n",
      "  New best model with AUROC: 0.7343\n",
      "Epoch 18/50 - Train Loss: 0.2660, Valid Loss: 0.2138\n",
      "Accuracy: 0.9414, Mean AUROC: 0.7333\n",
      "Epoch 19/50 - Train Loss: 0.2584, Valid Loss: 0.2289\n",
      "Accuracy: 0.9355, Mean AUROC: 0.7263\n",
      "Epoch 20/50 - Train Loss: 0.2567, Valid Loss: 0.2186\n",
      "Accuracy: 0.9388, Mean AUROC: 0.7278\n",
      "  Atelectasis: AUROC = 0.7016\n",
      "  Cardiomegaly: AUROC = 0.7802\n",
      "  Effusion: AUROC = 0.8256\n",
      "  Infiltration: AUROC = 0.6017\n",
      "  Mass: AUROC = 0.6202\n",
      "  Nodule: AUROC = 0.5364\n",
      "  Pneumonia: AUROC = 0.7641\n",
      "  Pneumothorax: AUROC = 0.8482\n",
      "  Consolidation: AUROC = 0.7133\n",
      "  Edema: AUROC = 0.8630\n",
      "  Emphysema: AUROC = 0.6608\n",
      "  Fibrosis: AUROC = 0.6840\n",
      "  Pleural_Thickening: AUROC = 0.6496\n",
      "  Hernia: AUROC = 0.9404\n",
      "Epoch 21/50 - Train Loss: 0.2496, Valid Loss: 0.2115\n",
      "Accuracy: 0.9440, Mean AUROC: 0.7298\n",
      "Epoch 22/50 - Train Loss: 0.2419, Valid Loss: 0.2105\n",
      "Accuracy: 0.9429, Mean AUROC: 0.7262\n",
      "Epoch 23/50 - Train Loss: 0.2376, Valid Loss: 0.2170\n",
      "Accuracy: 0.9384, Mean AUROC: 0.7243\n",
      "Epoch 24/50 - Train Loss: 0.2318, Valid Loss: 0.2111\n",
      "Accuracy: 0.9402, Mean AUROC: 0.7336\n",
      "Epoch 25/50 - Train Loss: 0.2234, Valid Loss: 0.2089\n",
      "Accuracy: 0.9417, Mean AUROC: 0.7215\n",
      "  Atelectasis: AUROC = 0.6988\n",
      "  Cardiomegaly: AUROC = 0.7512\n",
      "  Effusion: AUROC = 0.8352\n",
      "  Infiltration: AUROC = 0.6037\n",
      "  Mass: AUROC = 0.6480\n",
      "  Nodule: AUROC = 0.5403\n",
      "  Pneumonia: AUROC = 0.5983\n",
      "  Pneumothorax: AUROC = 0.8237\n",
      "  Consolidation: AUROC = 0.7522\n",
      "  Edema: AUROC = 0.8707\n",
      "  Emphysema: AUROC = 0.7142\n",
      "  Fibrosis: AUROC = 0.6718\n",
      "  Pleural_Thickening: AUROC = 0.6107\n",
      "  Hernia: AUROC = 0.9821\n",
      "  Early stopping triggered after 25 epochs\n",
      "Loaded best model with AUROC: 0.7343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.6150076894563892,\n",
       "  0.4905262444403622,\n",
       "  0.42387798437904584,\n",
       "  0.3856518385443697,\n",
       "  0.3633594352131266,\n",
       "  0.35329419158976155,\n",
       "  0.34079630753420936,\n",
       "  0.33410104391199413,\n",
       "  0.31920621483010475,\n",
       "  0.31799494007309015,\n",
       "  0.30749466005956555,\n",
       "  0.3003865083427762,\n",
       "  0.2942822369618837,\n",
       "  0.2919568213676514,\n",
       "  0.28467885411000315,\n",
       "  0.2761934056013576,\n",
       "  0.27163809748266926,\n",
       "  0.26600651799457076,\n",
       "  0.25842904915091447,\n",
       "  0.2566547337243406,\n",
       "  0.2495752162475455,\n",
       "  0.24194228084862796,\n",
       "  0.23764604326230043,\n",
       "  0.23177117166365171,\n",
       "  0.2234396956398951],\n",
       " 'valid_loss': [0.5666714046796163,\n",
       "  0.4569898856480916,\n",
       "  0.371339679479599,\n",
       "  0.3343385257720947,\n",
       "  0.2942291405200958,\n",
       "  0.278245933453242,\n",
       "  0.25931656138102216,\n",
       "  0.25193688142299653,\n",
       "  0.24427994549274445,\n",
       "  0.23331503105163573,\n",
       "  0.22846508328119913,\n",
       "  0.22407191602389018,\n",
       "  0.22131952675183614,\n",
       "  0.21914870476722717,\n",
       "  0.2200619997183482,\n",
       "  0.22248999659220378,\n",
       "  0.22964911778767905,\n",
       "  0.21384362451235453,\n",
       "  0.22886449948946636,\n",
       "  0.21856773742039998,\n",
       "  0.2115029188791911,\n",
       "  0.21054298710823058,\n",
       "  0.21701028501987457,\n",
       "  0.21108198753992716,\n",
       "  0.20893272856871287],\n",
       " 'valid_accuracy': [0.9497142857142857,\n",
       "  0.9507619047619048,\n",
       "  0.9498095238095238,\n",
       "  0.9507619047619048,\n",
       "  0.9499047619047619,\n",
       "  0.9492380952380952,\n",
       "  0.9488571428571428,\n",
       "  0.9480952380952381,\n",
       "  0.9464761904761905,\n",
       "  0.9471428571428572,\n",
       "  0.9457142857142857,\n",
       "  0.9474285714285714,\n",
       "  0.9443809523809524,\n",
       "  0.9477142857142857,\n",
       "  0.9416190476190476,\n",
       "  0.9427619047619048,\n",
       "  0.939047619047619,\n",
       "  0.9414285714285714,\n",
       "  0.9355238095238095,\n",
       "  0.9387619047619048,\n",
       "  0.944,\n",
       "  0.9428571428571428,\n",
       "  0.9383809523809524,\n",
       "  0.9401904761904762,\n",
       "  0.9417142857142857],\n",
       " 'best_auroc': np.float64(0.734284917878435)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.to(device)\n",
    "trainer(new_model, criterion, optimizer, trainloader, validloader, epochs=50, patience=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
